{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jnoqRMjo6CY1tQMfDw48XnUMrN4f_62M",
      "authorship_tag": "ABX9TyP+C20yOelToN4Y5T3VroYi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhirajs16/Code_Basics_Practice/blob/main/CB_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Multivariate Linear Regression**"
      ],
      "metadata": {
        "id": "2lHILlT6fE57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "!pip install word2number\n",
        "from word2number import w2n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jD1ye0LgSrD",
        "outputId": "2a38eacb-7849-49e6-c6b0-6d0ab0162ee6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=01808c9f9b0dbdb489983e8702e43f1e159e863756c54c4da6d1112b02cf7ab4\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get data\n",
        "df = pd.read_csv('/content/drive/MyDrive/CB/datasets/hiring.csv')\n",
        "\n",
        "\n",
        "# preprocessing\n",
        "df['experience'] = df['experience'].fillna('zero')\n",
        "df['experience'] = df['experience'].apply(w2n.word_to_num)\n",
        "\n",
        "df['test_score(out of 10)'] = df['test_score(out of 10)'].fillna(int(df['test_score(out of 10)'].mean()))\n",
        "\n",
        "X = df.drop(['salary($)'], axis = 1)\n",
        "y = df['salary($)']\n",
        "\n",
        "\n",
        "# fit model\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "model.predict([[2, 9, 6]]), model.predict([[12, 10, 10]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5W1ZsrdhXOU",
        "outputId": "39f854d5-780c-429a-e936-3d4952c61ac9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([53713.86677124]), array([93747.79628651]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Descent**\n",
        "\n",
        "Gradient Descent is an optimization algorithm used in machine learning (ML) and deep learning (DL) to minimize the loss function. It's like navigating a hill to find the lowest point.\n",
        "\n",
        "**How it works:** Imagine you're on a mountain in dense fog, trying to find the lowest point. Gradient Descent is like taking small steps in the direction that slopes downward the steepest. Each step gets you closer to the valley (the minimum of the loss function).\n",
        "\n",
        "### **Learning Rate**\n",
        "\n",
        "The learning rate is a hyperparameter that controls how big those steps are in Gradient Descent. (Eg. step = 0.01)\n",
        "\n",
        "**How it works:** If the steps (learning rate) are too large, you might overshoot the minimum and bounce around it. If the steps are too small, it will take a very long time to reach the bottom. The learning rate needs to be just right to efficiently find the minimum.\n",
        "\n",
        "### **Loss Function**\n",
        "The loss function (cost) measures how well your ML or DL model performs. It's a way to quantify the error between the predicted values and the actual values.\n",
        "\n",
        "**How it works:** Think of the loss function as the height of the mountain you're trying to descend. The higher the value, the worse your model is performing. Gradient Descent helps you minimize this loss, effectively guiding your model to perform better.\n",
        "\n",
        "**Differences**\n",
        "\n",
        "- Gradient Descent is the method used to minimize the loss function.\n",
        "\n",
        "- Learning Rate determines the size of the steps taken during Gradient Descent.\n",
        "\n",
        "- Loss Function is what Gradient Descent aims to minimize."
      ],
      "metadata": {
        "id": "TorgA65Qn82N"
      }
    }
  ]
}